---
title: "Exploratory Data Analysis for Data Science Capstone"
author: "Matthew Farwell"
output:
  html_document:
#  pdf_document:
    keep_md: true
documentclass: article
classoption: a4paper
subparagraph: yes
geometry: margin=1.5cm
header-includes: |
  \usepackage{titlesec}
  \titlespacing{\section}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
  \titlespacing{\subsection}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
  \titlespacing{\subsubsection}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
---

# Overview

This document contains the exploratory data analysis for the [Data Science Capstone](https://www.coursera.org/learn/data-science-project) milestone project.

The final product of this course will be a Shiny App capable of predicting the possible next words that a user could type.

This document shows the steps taken to analyse the data contained in the corpus of text provided as part of the project [https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

# Executive Summary

- The corpus consists of four languages
- Each language has three sources, twitter, blogs and new articles
- The corpus is very large (> 2 million English tweets for instance)


## Next steps

Once the final analysis of the data is completed, we will create a predictive model, based upon a subset of the data. We can then test the accuracy of the model against the test data. We will create a shiny application which will demonstrate the predictive model.

```{r, echo=FALSE, message=FALSE}
suppressPackageStartupMessages(suppressWarnings({
library(ggplot2)
library(knitr)
require(graphics)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
}))
```

```{css, echo=FALSE}
table {
  margin: auto;
  border-top: 1px solid #666;
  border-bottom: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
img {
	width: 1000px;
}
```

# Data Summary

The data consists of four languages, US English, German, Russian and Finnish. This analysis does not include the Russian files. For each language, we have three sets of documents (a corpus), for documents which come from blogs, news articles and twitter. Each document is a single line in the file.

The data is processed using a combination of bash scripts and R, using the quanteda package for ngram processing. For the source code, see [https://github.com/matthewfarwell/CapstoneExploratoryAnalysis](https://github.com/matthewfarwell/CapstoneExploratoryAnalysis).

```{r, echo=FALSE, message=FALSE}
documentSummary <- read.csv("gen/documents.csv")

kd <- documentSummary %>%
      relocate(c("lang", "type")) %>%
      filter(type %in% c("blogs", "news", "twitter"))

kable(subset(kd), col.names=c("Language", "Document Type", "Number of Documents"))
```

We split the documents into ngrams using the *tokens_ngrams* function from quanteda. We split into 1, 2 and 3-ngrams (words, bigrams and trigrams). The following is a summary of the three languages.

## Number of unique words

```{r, echo=FALSE, message=FALSE}
kd <- documentSummary %>%
      relocate(c("lang")) %>%
      filter(type == "ngrams_1") %>%
      select(-type)

kable(subset(kd), col.names=c("Language", "Number of Unique Words"))
```

## Number of unique bigrams

```{r, echo=FALSE, message=FALSE}
kd <- documentSummary %>%
      relocate(c("lang")) %>%
      filter(type == "ngrams_2") %>%
      select(-type)

kable(subset(kd), col.names=c("Language", "Number of Unique Bigrams"))
```

## Number of unique trigrams

```{r, echo=FALSE, message=FALSE}
kd <- documentSummary %>%
      relocate(c("lang")) %>%
      filter(type == "ngrams_3") %>%
      select(-type)

kable(subset(kd), col.names=c("Language", "Number of Unique Trigrams"))
```

```{r, echo=FALSE, message=FALSE}
nice_ngram <- function(s) {
	gsub("_", " ", s)
}
```


```{r, echo=FALSE, message=FALSE}
numberOfWords <- 300
ngrams = c(1,2,3)
enCsvs <- sapply(ngrams, function(n) {read.csv(file.path("gen", paste("en_US", ".", as.character(n), ".csv", sep="")), nrows=numberOfWords)})
deCsvs <- sapply(ngrams, function(n) {read.csv(file.path("gen", paste("de_DE", ".", as.character(n), ".csv", sep="")), nrows=numberOfWords)})

word_table <- function(csv, name) {
  words <- as.data.frame(csv) %>%
    mutate(ngram = nice_ngram(ngram)) %>%
    relocate(c("ngram"))
  
  #kable(words[1:15,], col.names=c(name, "Occurrences"))
  ggplot(words[1:15,], aes(ngram, n)) + geom_bar(stat="identity") + theme(axis.text.x=element_text(angle=45, hjust=1)) + xlab(name) + ylab("Frequency")
}

word_cloud <- function(csv) {
  words <- as.data.frame(csv) %>%
    mutate(n = n %/% 1000) %>%
    relocate(c("ngram"))
  
  set.seed(1234)
  wordcloud(words$ngram, words$n, min.freq=100, rot.per = 0, colors=brewer.pal(6, "Dark2"))
}
```

## Top 15 most frequent words in English

```{r, echo=FALSE, message=FALSE}
word_table(enCsvs[,1], "Word")
word_cloud(enCsvs[,1])
```

## Top 15 most frequent bigrams in English

```{r, echo=FALSE, message=FALSE}
word_table(enCsvs[,2], "Bigram")
```

## Top 15 most frequent trigrams in English

```{r, echo=FALSE, message=FALSE}
word_table(enCsvs[,3], "Trigram")
```

## Top 15 most frequent trigrams in German

We can similarly do the same for Finnish and German - here are the most frequent trigrams in German

```{r, echo=FALSE, message=FALSE}
word_table(deCsvs[,3], "Trigram")
```


# Appendix 1: Initial specification

## Tasks to accomplish

- Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora. 
- Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

# Questions to consider

- Some words are more frequent than others - what are the distributions of word frequencies? 
- What are the frequencies of 2-grams and 3-grams in the dataset? 
- How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? 
- How do you evaluate how many of the words come from foreign languages? 
- Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

## Instructions

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to:

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app. 


## Review criteria

- Does the link lead to an HTML page describing the exploratory analysis of the training data set?
- Has the data scientist done basic summaries of the three files? Word counts, line counts and basic data tables?
- Has the data scientist made basic plots, such as histograms to illustrate features of the data?
- Was the report written in a brief, concise style, in a way that a non-data scientist manager could appreciate?

# Appendix 2: Session Info

```{r, echo=TRUE}
sessionInfo()
```


